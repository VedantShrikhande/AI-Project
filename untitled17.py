# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1522hFoEd_8xXeW4RYP_unJriyZMvWXP7
"""

import re
import pandas as pd
import spacy
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from transformers import pipeline
import streamlit as st
import logging

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit

# ----------- PII Filtering (Regex + NER) -----------
PII_PATTERNS = {
    'email': re.compile(r'\b[\w\.-]+?@\w+?\.\w{2,4}\b'),
    'phone': re.compile(r'\b(?:\+\d{1,3})?[\s\-]?\(?\d{2,4}\)?[\s\-]?\d{3,5}[\s\-]?\d{4,6}\b'),
    'ssn': re.compile(r'\b\d{3}-\d{2}-\d{4}\b'),
    'credit_card': re.compile(r'\b(?:\d[ -]*?){13,16}\b'),
    'address': re.compile(r'\b\d{1,5}\s[\w\s]{1,20}(?:Street|St|Avenue|Ave|Blvd|Boulevard|Rd|Road|Lane|Ln|Drive|Dr)\b', re.I),
}

def detect_pii_regex(text):
    return [entity for entity, pattern in PII_PATTERNS.items() if pattern.search(text)]

nlp = spacy.load("en_core_web_sm")
def detect_pii_ner(text):
    doc = nlp(text)
    return [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in {"PERSON", "GPE", "ORG"}]

def contains_pii(text):
    results = detect_pii_regex(text)
    entities = detect_pii_ner(text)
    return bool(results or entities)

# ----------- Hate Speech Filtering (ML Classifier) -----------
@st.cache_resource
def load_hate_classifier():
    df = pd.read_csv('HateSpeechDataset.csv')  # Path to Kaggle dataset
    X = df['Content']
    y = df['label']
    vectorizer = CountVectorizer(stop_words='english')
    X_vec = vectorizer.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)
    clf = LogisticRegression()
    clf.fit(X_train, y_train)
    return vectorizer, clf

vectorizer, clf = load_hate_classifier()

def is_hate_speech(text):
    return clf.predict(vectorizer.transform([text]))[0] == 1

# ----------- LLM Integration -----------
@st.cache_resource
def load_llm():
    return pipeline("text-generation", model="meta-llama/Llama-2-7b-chat")

chatbot = load_llm()

def llm_response(prompt):
    return chatbot(prompt, max_new_tokens=150)[0]['generated_text']

def is_unsafe(text):
    return contains_pii(text) or is_hate_speech(text)

# ----------- Streamlit Frontend with Logging -----------
logger = logging.getLogger("chatbot_logger")
logger.setLevel(logging.INFO)
if not logger.hasHandlers():
    handler = logging.FileHandler('chatbot_safety_log.txt')
    formatter = logging.Formatter("%(asctime)s %(levelname)s: %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)

st.title("Secure LLM Chatbot Demo")

if "messages" not in st.session_state:
    st.session_state["messages"] = []

for message in st.session_state["messages"]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Say something..."):
    st.session_state["messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    response = llm_response(prompt)
    if is_unsafe(response):
        safe_response = "Sorry, this response was filtered for safety. Please rephrase your request."
        logger.info(f"BLOCKED | Prompt: '{prompt}' | Response: '{response}'")
    else:
        safe_response = response
        logger.info(f"SAFE | Prompt: '{prompt}' | Response: '{response}'")
    with st.chat_message("assistant"):
        st.markdown(safe_response)
    st.session_state["messages"].append({"role": "assistant", "content": safe_response})